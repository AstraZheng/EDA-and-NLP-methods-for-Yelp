{"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3316532,"sourceType":"datasetVersion","datasetId":10100},{"sourceId":7738255,"sourceType":"datasetVersion","datasetId":4522673},{"sourceId":7765671,"sourceType":"datasetVersion","datasetId":4542285},{"sourceId":7766637,"sourceType":"datasetVersion","datasetId":4543000},{"sourceId":8017567,"sourceType":"datasetVersion","datasetId":4723950}],"dockerImageVersionId":30732,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Package loading","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport json\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom scipy import stats\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom PIL import Image\nfrom sklearn import preprocessing\nimport string\nimport langid\nfrom textblob import TextBlob\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom collections import Counter\nimport langid\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n","metadata":{"execution":{"iopub.execute_input":"2024-04-03T08:25:56.439859Z","iopub.status.busy":"2024-04-03T08:25:56.439375Z","iopub.status.idle":"2024-04-03T08:25:58.778467Z","shell.execute_reply":"2024-04-03T08:25:58.776960Z","shell.execute_reply.started":"2024-04-03T08:25:56.439812Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","output_type":"stream","text":"/kaggle/input/review-subjectivity-ordered/Review - Subjectivity.csv\n\n/kaggle/input/review-polarity/Review - Polarity - 1.csv\n\n/kaggle/input/review-polarity/Review - Polarity - 6.csv\n\n/kaggle/input/review-polarity/Review - Polarity - 4.csv\n\n/kaggle/input/review-polarity/Review - Polarity - 7.csv\n\n/kaggle/input/review-polarity/Review - Polarity - 3.csv\n\n/kaggle/input/review-polarity/Review - Polarity - 5.csv\n\n/kaggle/input/review-polarity/Review - Polarity - 2.csv\n\n/kaggle/input/non-english-review-id/Non English Review ID.csv\n\n/kaggle/input/yelp-dataset/Dataset_User_Agreement.pdf\n\n/kaggle/input/yelp-dataset/yelp_academic_dataset_review.json\n\n/kaggle/input/yelp-dataset/yelp_academic_dataset_checkin.json\n\n/kaggle/input/yelp-dataset/yelp_academic_dataset_business.json\n\n/kaggle/input/yelp-dataset/yelp_academic_dataset_tip.json\n\n/kaggle/input/yelp-dataset/yelp_academic_dataset_user.json\n"},{"name":"stderr","output_type":"stream","text":"/opt/conda/lib/python3.10/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n\n  warnings.warn(\"The twython library has not been installed. \"\n"}]},{"cell_type":"markdown","source":"# 2.Data loading","metadata":{}},{"cell_type":"markdown","source":"## 2.1 Business data","metadata":{}},{"cell_type":"code","source":"with open('/kaggle/input/yelp-dataset/yelp_academic_dataset_business.json') as data_file:\n    data = [json.loads(line) for line in data_file]\ndf_business_final = pd.DataFrame(data)\n\ndf_business_final.drop(['address', 'hours'], axis=1, inplace=True)","metadata":{"execution":{"iopub.execute_input":"2024-04-03T08:25:58.781798Z","iopub.status.busy":"2024-04-03T08:25:58.781126Z","iopub.status.idle":"2024-04-03T08:26:04.722721Z","shell.execute_reply":"2024-04-03T08:26:04.721302Z","shell.execute_reply.started":"2024-04-03T08:25:58.781756Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def clean_business_data(df_cleaned):\n    \n    # address the missing values\n    df_cleaned['attributes'].fillna('unknown', inplace=True)\n    df_cleaned['categories'].fillna('未知', inplace=True)\n    df_cleaned['review_count'].fillna(df_cleaned['review_count'].median(), inplace=True)\n    df_cleaned['is_open'].fillna(0, inplace=True)\n\n    # deal with missing postal code\n    df_notnull = df_cleaned.dropna(subset=['postal_code'])\n    for index, row in df_cleaned[df_cleaned['postal_code'].isnull()].iterrows():\n        distances = df_notnull.apply(lambda x: abs(row['latitude'] - x['latitude']) + abs(row['longitude'] - x['longitude']), axis=1)\n        nearest_index = distances.idxmin()\n        df_cleaned.at[index, 'postal_code'] = df_notnull.at[nearest_index, 'postal_code']\n\n    # check and address the outlier\n    df_cleaned = df_cleaned[df_cleaned['latitude'].between(-90, 90) & df_cleaned['longitude'].between(-180, 180)]\n    df_cleaned = df_cleaned[df_cleaned['stars'].between(0, 5)]\n    \n    # rename the column\n    df_cleaned = df_cleaned.rename(columns = {'stars' : 'business_stars',\n                                              'name' : 'business_name'})\n\n    return df_cleaned\n\ndf_business_cleaned = clean_business_data(df_business_final)","metadata":{"execution":{"iopub.execute_input":"2024-04-03T08:26:04.724943Z","iopub.status.busy":"2024-04-03T08:26:04.724527Z","iopub.status.idle":"2024-04-03T08:26:05.092153Z","shell.execute_reply":"2024-04-03T08:26:05.090895Z","shell.execute_reply.started":"2024-04-03T08:26:04.724908Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"### Filter the categories to restaurant","metadata":{}},{"cell_type":"code","source":"restaurant_ids = []\n\nfor i in range(0, len(df_business_cleaned)):\n    string_list = df_business_cleaned['categories'][i].lower().split()\n    if ('restaurants' in string_list) | ('restaurants,' in string_list):\n        restaurant_ids.append(df_business_cleaned['business_id'][i])\n# filter the business dataset by restaurant\ndf_business_restaurant = df_business_cleaned[df_business_cleaned['business_id'].isin(restaurant_ids)]","metadata":{"execution":{"iopub.execute_input":"2024-04-03T08:26:05.096071Z","iopub.status.busy":"2024-04-03T08:26:05.095555Z","iopub.status.idle":"2024-04-03T08:26:07.784037Z","shell.execute_reply":"2024-04-03T08:26:07.782779Z","shell.execute_reply.started":"2024-04-03T08:26:05.096028Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## 2.2 Review data","metadata":{}},{"cell_type":"code","source":"reviews = pd.read_json('/kaggle/input/yelp-dataset/yelp_academic_dataset_review.json', lines=True, orient='columns', chunksize=10000)\n\ndf_review = []\nfor chunk in reviews:\n  df_review.append(chunk)\n\ndf_reviews = pd.concat(df_review, ignore_index=True)\ndf_reviews['stars'] = df_reviews['stars'].astype(object)\ndf_reviews['text'] = df_reviews['text'].astype('str')\n\ndf_id = pd.read_csv('/kaggle/input/non-english-review-id/Non English Review ID.csv')\nid_non_en = df_id['review_id']\n","metadata":{"execution":{"iopub.execute_input":"2024-04-03T08:26:07.786540Z","iopub.status.busy":"2024-04-03T08:26:07.786067Z","iopub.status.idle":"2024-04-03T08:31:17.565160Z","shell.execute_reply":"2024-04-03T08:31:17.563148Z","shell.execute_reply.started":"2024-04-03T08:26:07.786499Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"### Detect and remove the non-English Review","metadata":{}},{"cell_type":"code","source":"id_non_en = []\nfor i in range(0,700):\n  for j in range(0, len(df_review[i])):\n    text = df_review[i]['text']\n    lang = langid.classify(text.iloc[j,])[0]\n    if lang != 'en':\n      id_non_en.append(df_review[i].iloc[j,]['review_id'])\ndf_non_en_review = pd.DataFrame({'review_id': id_non_en})\ndf_non_en_review.to_csv('Non English Review ID.csv')\n\n# load the data\ndf_id = pd.read_csv('/kaggle/input/non-english-review-id/Non English Review ID.csv')\nid_non_en = df_id['review_id']\n\n# keep the English review only\ndf_en_review = df_reviews[~ df_reviews['review_id'].isin(id_non_en)]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check the duplication\ndf_review_unique = df_en_review.drop_duplicates(subset = ['review_id'], keep = 'first')\n# remove the duplicate records with the same review contents to the same business (keep the first record)\ndf_review_cleaned = df_review_unique.drop_duplicates(subset = ['text', 'user_id', 'business_id'], keep = 'first')\n\n# rename the columns\ndf_review_cleaned = df_review_cleaned.rename(columns = {'useful' : 'review_useful',\n                                                        'funny' : 'review_funny',\n                                                        'cool' : 'review_cool'})\n\n# sort the reviews by time \ndf_review_all = df_review_cleaned.sort_values(by = ['date'], ascending = False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Detect the sentiment of reviews: polarity and subjectivity","metadata":{}},{"cell_type":"code","source":"# review polarity\npolarity = []\nsentiment = SentimentIntensityAnalyzer()\nfor i in range(0, len(df_review_cleaned)):\n    text =  df_review_cleaned['text'].iloc[i]\n    senti = sentiment.polarity_scores(text)\n    polarity.append(senti['compound'])\ndf_polarity = pd.DataFrame({'polarity': polarity})\ndf_polarity.to_csv('Review - Polarity.csv') \n\n# add polarity\npaths = ['/kaggle/input/review-polarity/Review - Polarity - 1.csv', '/kaggle/input/review-polarity/Review - Polarity - 2.csv',\n         '/kaggle/input/review-polarity/Review - Polarity - 3.csv', '/kaggle/input/review-polarity/Review - Polarity - 4.csv',\n         '/kaggle/input/review-polarity/Review - Polarity - 5.csv', '/kaggle/input/review-polarity/Review - Polarity - 6.csv',\n         '/kaggle/input/review-polarity/Review - Polarity - 7.csv']\npolarity = []\nfor i in range(0,7):\n    polarity = polarity + pd.read_csv(paths[i])['polarity'].tolist()\n    \ndf_review_all['rev_polarity'] = polarity","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# review subjectivity\nsubjectivity = []\nfor i in range(0, len(df_review_cleaned)):\n    text =  df_review_cleaned['text'].iloc[i]\n    subjectivity.append(TextBlob(text).sentiment.subjectivity)   \ndf_subjectivity = pd.DataFrame({'subjectivity': subjectivity})\ndf_subjectivity.to_csv('Review - Subjectivity.csv')\n\n# add subjectivity\nsubjectivity = pd.read_csv('/kaggle/input/review-subjectivity-ordered/Review - Subjectivity.csv')['rev_subjectivity'].tolist()\ndf_review_all['rev_subjectivity'] = subjectivity","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Filter the categories to restaurant","metadata":{}},{"cell_type":"code","source":"# filter the review dataset to restaurants\ndf_review_restaurant = df_review_all[df_review_cleaned['business_id'].isin(restaurant_ids)]\n\n# add the year of review\ndf_review_restaurant['year'] = df_review_restaurant['date'].dt.year\n\n# add the text length to restaurant review\ntexts = df_review_restaurant['text'].reset_index()\ntext_length = []\nfor i in range(0, len(texts)):\n  text_length.append(len(texts['text'][i].split()))\ndf_review_restaurant['review_length'] = text_length","metadata":{"execution":{"iopub.execute_input":"2024-04-03T08:31:17.568973Z","iopub.status.busy":"2024-04-03T08:31:17.568573Z","iopub.status.idle":"2024-04-03T08:33:01.885361Z","shell.execute_reply":"2024-04-03T08:33:01.884063Z","shell.execute_reply.started":"2024-04-03T08:31:17.568936Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# If need lastest 100W reviews\ndf_review_restaurant_100w = df_review_restaurant.iloc[0:1000000,]","metadata":{"execution":{"iopub.execute_input":"2024-04-03T08:33:01.887797Z","iopub.status.busy":"2024-04-03T08:33:01.887183Z","iopub.status.idle":"2024-04-03T08:33:01.894679Z","shell.execute_reply":"2024-04-03T08:33:01.893405Z","shell.execute_reply.started":"2024-04-03T08:33:01.887749Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## 2.3 User data","metadata":{}},{"cell_type":"code","source":"user = pd.read_json('/kaggle/input/yelp-dataset/yelp_academic_dataset_user.json', lines=True, orient='columns', chunksize=10000)\n\ndf_user = []\nfor chunk in user:\n  df_user.append(chunk)\n\ndf_user_final = pd.concat(df_user, ignore_index=True)","metadata":{"execution":{"iopub.execute_input":"2024-04-03T08:33:01.896971Z","iopub.status.busy":"2024-04-03T08:33:01.896550Z","iopub.status.idle":"2024-04-03T08:35:45.753301Z","shell.execute_reply":"2024-04-03T08:35:45.751707Z","shell.execute_reply.started":"2024-04-03T08:33:01.896937Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def clean_user_data(df):\n\n    # delete the duplicated rows\n    df_cleaned=df.drop_duplicates()\n\n    # switch the column 'yelping_since' to date type\n    df_cleaned['yelping_since'] = pd.to_datetime(df_cleaned['yelping_since'])\n\n    # merge compliment columns\n    compliment_columns = [col for col in df_cleaned.columns if col.startswith('compliment_')]\n    df_cleaned['compliment_total'] = df_cleaned[compliment_columns].sum(axis=1)\n\n    # detect and remove the outlier\n    df_cleaned = df_cleaned[df_cleaned['average_stars'].between(0, 5)]\n\n    # count the total number of friends \n    df_cleaned['friends_count'] = df_cleaned['friends'].fillna('[]').str.count(',') + 1\n    df_cleaned['friends_count'] = df_cleaned.apply(lambda x: 0 if x['friends'] == '[]' else x['friends_count'], axis=1)\n\n    # count the total number of elite years\n    #df_cleaned['elite'] = df_cleaned['elite'].replace('', 'No')\n    df_cleaned['elites_count'] = df_cleaned['elite'].str.count(',') + 1\n    df_cleaned['elites_count'] = df_cleaned.apply(lambda x: 0 if x['elite'] == '' else x['elites_count'], axis=1)\n    \n    # rename the columns \n    df_cleaned = df_cleaned.rename(columns = {'useful' : 'user_useful',\n                                              'funny' : 'user_funny',\n                                              'cool' : 'user_cool',\n                                              'name' : 'user_name'})\n    \n    return df_cleaned\n\ndf_user_cleaned=clean_user_data(df_user_final)","metadata":{"execution":{"iopub.execute_input":"2024-04-03T08:35:45.758572Z","iopub.status.busy":"2024-04-03T08:35:45.757303Z","iopub.status.idle":"2024-04-03T08:37:43.990589Z","shell.execute_reply":"2024-04-03T08:37:43.989182Z","shell.execute_reply.started":"2024-04-03T08:35:45.758524Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# filter the user dataset by restaurant\nrev_num = df_review_restaurant.groupby(['user_id']).agg({'review_id':'count', 'stars':'mean', 'rev_polarity': 'mean'}).reset_index()\nonlyRestaurantReviewCount = rev_num[['user_id','review_id','stars','rev_polarity']].rename(columns = {'review_id':'review_count','stars':'average_stars'})\n# define active / non-active user based on the restaurant review\nonlyRestaurantReviewCount['user_activity'] = 'Non-active User'\nonlyRestaurantReviewCount.loc[onlyRestaurantReviewCount['review_count'] > 1, 'user_activity'] = 'Active User'\nuserIdRestaurant = df_review_restaurant['user_id'].unique().tolist()\ndf_user_restaurant = df_user_cleaned[df_user_cleaned['user_id'].isin(userIdRestaurant)]\ndf_user_restaurant= df_user_restaurant.drop(['review_count', 'average_stars'], axis=1)\n##df_user_restaurant = df_user_restaurant[['user_id','user_name','yelping_since', 'friends_count','elites_count', 'user_type', 'user_famous', 'elite_status']]\ndf_user_restaurant = df_user_restaurant.merge(onlyRestaurantReviewCount, on = ['user_id'], how = 'left')","metadata":{"execution":{"iopub.execute_input":"2024-04-03T08:37:43.994868Z","iopub.status.busy":"2024-04-03T08:37:43.994462Z","iopub.status.idle":"2024-04-03T08:41:08.888935Z","shell.execute_reply":"2024-04-03T08:41:08.887435Z","shell.execute_reply.started":"2024-04-03T08:37:43.994835Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## 2.4 Merge three datasets","metadata":{}},{"cell_type":"code","source":"# merge the three datasets\ndf_all = df_review_restaurant.merge(df_business_restaurant, on = ['business_id'], how = 'left').merge(df_user_restaurant, on = ['user_id'], how = 'left') ","metadata":{"execution":{"iopub.execute_input":"2024-04-03T08:41:08.891780Z","iopub.status.busy":"2024-04-03T08:41:08.891369Z","iopub.status.idle":"2024-04-03T08:41:53.381616Z","shell.execute_reply":"2024-04-03T08:41:53.380120Z","shell.execute_reply.started":"2024-04-03T08:41:08.891743Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"df_100w = df_all.iloc[0:1000000, ]","metadata":{"execution":{"iopub.execute_input":"2024-04-03T08:41:53.384987Z","iopub.status.busy":"2024-04-03T08:41:53.383977Z","iopub.status.idle":"2024-04-03T08:41:53.392102Z","shell.execute_reply":"2024-04-03T08:41:53.390574Z","shell.execute_reply.started":"2024-04-03T08:41:53.384926Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"df_100w.groupby(['stars', 'user_activity']).agg({'review_length': 'mean'})","metadata":{"execution":{"iopub.execute_input":"2024-04-03T04:08:35.516525Z","iopub.status.busy":"2024-04-03T04:08:35.516055Z","iopub.status.idle":"2024-04-03T04:08:35.956271Z","shell.execute_reply":"2024-04-03T04:08:35.954817Z","shell.execute_reply.started":"2024-04-03T04:08:35.516490Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th></th>\n","      <th>review_length</th>\n","    </tr>\n","    <tr>\n","      <th>stars</th>\n","      <th>user_activity</th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th rowspan=\"2\" valign=\"top\">1</th>\n","      <th>Active User</th>\n","      <td>107.691146</td>\n","    </tr>\n","    <tr>\n","      <th>Non-active User</th>\n","      <td>102.981694</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"2\" valign=\"top\">2</th>\n","      <th>Active User</th>\n","      <td>118.674185</td>\n","    </tr>\n","    <tr>\n","      <th>Non-active User</th>\n","      <td>105.265649</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"2\" valign=\"top\">3</th>\n","      <th>Active User</th>\n","      <td>115.976773</td>\n","    </tr>\n","    <tr>\n","      <th>Non-active User</th>\n","      <td>95.462004</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"2\" valign=\"top\">4</th>\n","      <th>Active User</th>\n","      <td>103.021282</td>\n","    </tr>\n","    <tr>\n","      <th>Non-active User</th>\n","      <td>69.928466</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"2\" valign=\"top\">5</th>\n","      <th>Active User</th>\n","      <td>77.959408</td>\n","    </tr>\n","    <tr>\n","      <th>Non-active User</th>\n","      <td>56.297571</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                       review_length\n","stars user_activity                 \n","1     Active User         107.691146\n","      Non-active User     102.981694\n","2     Active User         118.674185\n","      Non-active User     105.265649\n","3     Active User         115.976773\n","      Non-active User      95.462004\n","4     Active User         103.021282\n","      Non-active User      69.928466\n","5     Active User          77.959408\n","      Non-active User      56.297571"]},"metadata":{}}]},{"cell_type":"markdown","source":"## 2.5 Text Preprocessing and Word Embedding","metadata":{}},{"cell_type":"code","source":"import re\nimport spacy\nfrom time import time\nimport logging \nimport multiprocessing\nfrom gensim.models import Word2Vec\nfrom sklearn.feature_extraction.text import TfidfVectorizer","metadata":{"execution":{"iopub.execute_input":"2024-04-03T09:52:40.336013Z","iopub.status.busy":"2024-04-03T09:52:40.335148Z","iopub.status.idle":"2024-04-03T09:52:40.343279Z","shell.execute_reply":"2024-04-03T09:52:40.341641Z","shell.execute_reply.started":"2024-04-03T09:52:40.335975Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"df_sub = df_all.iloc[0:25000, ]","metadata":{"execution":{"iopub.execute_input":"2024-04-03T09:52:41.424992Z","iopub.status.busy":"2024-04-03T09:52:41.424554Z","iopub.status.idle":"2024-04-03T09:52:41.431537Z","shell.execute_reply":"2024-04-03T09:52:41.429543Z","shell.execute_reply.started":"2024-04-03T09:52:41.424956Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"# text preprocessing\nnlp = spacy.load('en_core_web_sm', disable=['ner', 'parser']) # disabling Named Entity Recognition for speed\ndef cleaning(doc):\n    # Lemmatizes and removes stopwords\n    # doc needs to be a spacy Doc object\n    txt = [token.lemma_ for token in doc if not token.is_stop]\n    # Word2Vec uses context words to learn the vector representation of a target word,\n    # if a sentence is only one or two words long, the benefit for the training is very small\n    if len(txt) > 2:\n        return ' '.join(txt).split()\n    \nbrief_cleaning = (re.sub(\"[^A-Za-z']+\", ' ', str(row)).lower() for row in df_sub['text'])\ntxt = [cleaning(doc) for doc in nlp.pipe(brief_cleaning, batch_size=5000)]\ndf_nlp_w = pd.DataFrame({'review_id': df_sub['review_id'],\n                         'nlp_clean_review': txt})\ndf_nlp_w = df_nlp_w.dropna(subset = ['nlp_clean_review']).drop_duplicates(['nlp_clean_review'])","metadata":{"execution":{"iopub.execute_input":"2024-04-03T09:52:41.968087Z","iopub.status.busy":"2024-04-03T09:52:41.967674Z","iopub.status.idle":"2024-04-03T09:57:09.794317Z","shell.execute_reply":"2024-04-03T09:57:09.792914Z","shell.execute_reply.started":"2024-04-03T09:52:41.968055Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"### Word2Vec","metadata":{}},{"cell_type":"code","source":"# Word embedding \n# sg: (default 0 or CBOW) The training algorithm, either CBOW (0) or skip gram (1).\n# min_count: The minimum count of words to consider when training the model; words with an occurrence less than this count will be ignored.\n# vector_size: (default 100) The number of dimensions of the embedding, e.g. the length of the dense vector to represent each token (word).\n# window: (default 5) The maximum distance between a target word and words around the target word.\n\nw2v = Word2Vec(df_nlp_w['nlp_clean_review'], window = 5, vector_size=200, min_count = 10, sg = 0)\n\n# get sentence vector by calculate the average word vectors and take the weight from tf-idf\ndata = df_nlp_w['nlp_clean_review']\navg_data = []\n\nfor row in data:       \n    vec = np.zeros(200)\n    count = 0\n    for word in row:\n        try:\n            if word in dict_tfidf:\n                vec += w2v.wv[word] * dict_tfidf[word]\n            else: vec += w2v.wv[word]\n            count += 1\n        except:\n            pass\n    avg_data.append(vec/count)  \n\ndf_nlp_v = pd.DataFrame(avg_data)\ndf_nlp_v['review_id'] = df_nlp_w['review_id']","metadata":{"execution":{"iopub.execute_input":"2024-04-03T09:57:09.797520Z","iopub.status.busy":"2024-04-03T09:57:09.796529Z","iopub.status.idle":"2024-04-03T09:57:09.804044Z","shell.execute_reply":"2024-04-03T09:57:09.802666Z","shell.execute_reply.started":"2024-04-03T09:57:09.797480Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"merge word2vec to main dataset\ndf_all = df_all.merge(df_nlp_v, on = ['review_id'], how = 'inner')","metadata":{"execution":{"iopub.execute_input":"2024-04-03T09:57:09.806268Z","iopub.status.busy":"2024-04-03T09:57:09.805845Z","iopub.status.idle":"2024-04-03T09:57:09.822172Z","shell.execute_reply":"2024-04-03T09:57:09.821276Z","shell.execute_reply.started":"2024-04-03T09:57:09.806229Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":"### Pre-trained GloVe","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","metadata":{"execution":{"iopub.execute_input":"2024-04-03T09:57:09.825854Z","iopub.status.busy":"2024-04-03T09:57:09.825352Z","iopub.status.idle":"2024-04-03T09:57:09.837746Z","shell.execute_reply":"2024-04-03T09:57:09.836552Z","shell.execute_reply.started":"2024-04-03T09:57:09.825811Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"#df_100w = pd.read_csv('df_100w_all_noWord2Vec_withText.csv', index_col = [0])","metadata":{"execution":{"iopub.execute_input":"2024-04-03T09:57:09.840344Z","iopub.status.busy":"2024-04-03T09:57:09.839501Z","iopub.status.idle":"2024-04-03T09:57:09.849980Z","shell.execute_reply":"2024-04-03T09:57:09.848316Z","shell.execute_reply.started":"2024-04-03T09:57:09.840307Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"# Generate a cleaned reviews array from original review texts\ndef clean_document(doco):\n    punctuation = string.punctuation + '\\n\\n';\n    punc_replace = ''.join([' ' for s in punctuation]);\n    doco_clean = doco.replace('-', ' ');\n    doco_alphas = re.sub(r'\\W +', '', doco_clean)\n    trans_table = str.maketrans(punctuation, punc_replace);\n    doco_clean = ' '.join([word.translate(trans_table) for word in doco_alphas.split(' ')]);\n    doco_clean = doco_clean.split(' ');\n    doco_clean = [word.lower() for word in doco_clean if len(word) > 0];\n    \n    return doco_clean;\n\n# Generate a cleaned reviews array from original review texts\nreview_cleans = [clean_document(row) for row in df_sub['text']];  ### change to df_100w\nsentences = [' '.join(r) for r in review_cleans]\n\ntokenizer = Tokenizer();\ntokenizer.fit_on_texts(sentences);\nsequence_dict = tokenizer.word_index;","metadata":{"execution":{"iopub.execute_input":"2024-04-03T09:57:09.852853Z","iopub.status.busy":"2024-04-03T09:57:09.852400Z","iopub.status.idle":"2024-04-03T09:57:16.628959Z","shell.execute_reply":"2024-04-03T09:57:16.627183Z","shell.execute_reply.started":"2024-04-03T09:57:09.852816Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"embeddings_index = dict();\nwith open('/kaggle/input/pre-trained-glove/glove.6B.200d.txt') as f:\n    for line in f:\n        values = line.split();\n        word = values[0];\n        coefs = np.asarray(values[1:], dtype='float32');\n        embeddings_index[word] = coefs;","metadata":{"execution":{"iopub.execute_input":"2024-04-03T09:57:16.631398Z","iopub.status.busy":"2024-04-03T09:57:16.630920Z","iopub.status.idle":"2024-04-03T09:57:43.441852Z","shell.execute_reply":"2024-04-03T09:57:43.439919Z","shell.execute_reply.started":"2024-04-03T09:57:16.631362Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"# using pre-trained GloVe\n# get sentence vector by calculate the average word vectors and take the weight from tf-idf \ndata = df_nlp_w['nlp_clean_review']\navg_data = []\n\nfor row in data:       \n    vec = np.zeros(200)\n    count = 0\n    for word in row:\n        try:\n            if word in dict_tfidf:\n                vec += embeddings_index.get(word) * dict_tfidf[word]\n            else: vec += embeddings_index.get(word)\n            count += 1\n        except:\n            pass\n    avg_data.append(vec/count)  \n\ndf_nlp_glove = pd.DataFrame(avg_data)\ndf_nlp_glove['review_id'] = df_nlp_w['review_id'].tolist()","metadata":{"execution":{"iopub.execute_input":"2024-04-03T09:57:43.444646Z","iopub.status.busy":"2024-04-03T09:57:43.444143Z","iopub.status.idle":"2024-04-03T09:57:52.354235Z","shell.execute_reply":"2024-04-03T09:57:52.352295Z","shell.execute_reply.started":"2024-04-03T09:57:43.444610Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":"## 2.6 Grouping users by single-review and multi-review","metadata":{}},{"cell_type":"code","source":"# multi-review users\ndf_active = df_all[df_all['user_activity'] == 'Active User']\n\ndf_non_active = df_all[df_all['user_activity'] == 'Non-active User']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Feature Engineering","metadata":{}},{"cell_type":"code","source":"import featuretools as ft\ndataframes = {\n    \"review\": (df_review_cleaned2, \"review_id\"),\n    \"business\": (df_business_cleaned, \"business_id\"),\n    \"user\": (df_user_cleaned, \"user_id\"),\n}\n\nrelationships = [\n    (\"business\", \"business_id\", \"review\", \"business_id\"),\n    (\"user\", \"user_id\", \"review\", \"user_id\"),\n]\nfeature_matrix, features_defs = ft.dfs(\n    dataframes=dataframes,\n    relationships=relationships,\n    target_dataframe_name=\"review\",\n)\nfeature_matrix.head()","metadata":{},"execution_count":null,"outputs":[]}]}